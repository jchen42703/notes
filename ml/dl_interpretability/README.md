# Deep Learning Interpretability

What questions do we need to ask when a model makes a prediction and we want to use it in production?
* Overfitting
* Bias
* Bugged performance
* CNNs
  * Where is the model focusing on?
* Tabular
  * Feature attribution
* How does adversarial noise impact model predictions?
* Groups of features?
  * i.e. with [t-SNE](https://distill.pub/2016/misread-tsne/)
* RNNs
  * https://distill.pub/2019/memorization-in-rnns/
---
* How do you interpret weight histograms?

* How do you interpret mid-NN outputs?

* How do you interpret LSTMs/RNNs?
